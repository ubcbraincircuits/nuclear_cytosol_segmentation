{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import skimage.filters as filters\n",
    "\n",
    "from PIL import Image\n",
    "#from pyneurotrace import filters  as pntfilters\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Folder containing sub-directories to include in analysis\n",
    "DATA_DIRECTORY = \"/home/islandhead/Documents/DBC_Tutoring/Wissam/osfstorage-archive/Data (Images)/\"\n",
    "# Frequency frames were collected\n",
    "HZ = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nColeman, P. (2019).\\npyNeuroTrace. https://github.com/padster/pyNeuroTrace.git\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Jia, H., Rochefort, N. L., Chen, X., & Konnerth, A. (2011).\n",
    "In vivo two-photon imaging of sensory-evoked dendritic calcium signals in cortical neurons.\n",
    "Nature protocols, 6(1), 28.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Podgorski, K., & Haas, K. (2013).\n",
    "Fast non‐negative temporal deconvolution for laser scanning microscopy.\n",
    "Journal of biophotonics, 6(2), 153-162.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Coleman, P. (2019).\n",
    "pyNeuroTrace. https://github.com/padster/pyNeuroTrace.git\n",
    "\"\"\"\n",
    "# To install pyneuortrace use this:\n",
    "# pip install --upgrade \"git+https://github.com/padster/pyNeuroTrace#egg=pyneurotrace&subdirectory=pyneurotrace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change output figure size\n",
    "# ...needs to be in its own cell for some reason...\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Performs fast nonnegative deconvolution on pmt signal to solve for minimum MSE photon rate\n",
    "   trace  :   The data to be deconvolved\n",
    "   tau    :   The time constant of the PMT, in data samples\n",
    "   return :   estimated photon rate\n",
    "A matlab version is also available on request.\n",
    "For details on how this works, see:\n",
    "  Podgorski, K., & Haas, K. (2013).\n",
    "  Fast non‐negative temporal deconvolution for laser scanning microscopy.\n",
    "  Journal of biophotonics, 6(2), 153-162.\n",
    "\"\"\"\n",
    "def nonNegativeDeconvolution(trace, tau):\n",
    "    T = len(trace)\n",
    "    counts = np.zeros(T)\n",
    "    counts[-1] = trace[-1]\n",
    "    cutoff = math.ceil(8 * tau)\n",
    "    kernel = np.exp(-np.arange(cutoff + 1)/tau) # convolution kernel\n",
    "    recent = np.full(1 + round(T / 2), np.nan).astype(int)\n",
    "    recent[0] = T #stored locations where we assigned counts\n",
    "    recentIdx = 0\n",
    "\n",
    "    # the points that could potentially be assigned counts:\n",
    "    _delayed = np.concatenate(([0], trace[:-2]))\n",
    "    points = (trace[:-1] > kernel[1] * _delayed) & (trace[:-1] > 0)\n",
    "\n",
    "    # dividing these points up into runs, for speed\n",
    "    runStarts = np.where(points & ~(np.concatenate(([False], points[:-1]))))[0].astype(int)\n",
    "    runEnds = np.where(points & ~(np.concatenate((points[1:], [False]))))[0].astype(int)\n",
    "    runIdx = len(runEnds) - 1\n",
    "\n",
    "    while runIdx >= 0:\n",
    "        oldTop, oldBottom = 0, 0\n",
    "        t = runEnds[runIdx]\n",
    "        t1 = t\n",
    "        accum = 0\n",
    "\n",
    "        converged = False\n",
    "        while not converged:\n",
    "            if recentIdx >= 0 and recent[recentIdx] < (t+cutoff):\n",
    "                t2 = recent[recentIdx] - 1\n",
    "                C_max = counts[t2] / kernel[t2-t]\n",
    "            else:\n",
    "                t2 = min(t + cutoff, T+1) - 1\n",
    "                C_max = np.inf\n",
    "\n",
    "\n",
    "            b = kernel[t1-t:t2-t]\n",
    "            top = np.dot(b, trace[t1:t2]) + oldTop #this is the numerator of the least squares fit for an exponential\n",
    "            bottom = np.dot(b, b) + oldBottom #this is the denominator of the fit\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                #the error function is (data-kernel.*C)^2\n",
    "                bestC = max(top/bottom, 0);  #C=top/bottom sets the derivative of the error to 0\n",
    "\n",
    "                # does not meet nonnegative constraint. Continue to adjust previous solutions.\n",
    "                if bestC > (C_max+accum):\n",
    "                    accum = accum + counts[t2] / kernel[t2-t]\n",
    "                    counts[t2] = 0\n",
    "                    t1 = t2\n",
    "                    oldTop = top\n",
    "                    oldBottom = bottom\n",
    "                    recentIdx -= 1\n",
    "                    done = True\n",
    "\n",
    "                else: # converged!\n",
    "                    #now that we have found the MSE counts for times t<end, check if\n",
    "                    #this will be swamped by the next timepoint in the run\n",
    "                    if  (t == runStarts[runIdx]) or (trace[t-1] < bestC/kernel[1]): #%C_max won't necessarily get swamped\n",
    "                        if recentIdx >= 0 and t2 <= t + cutoff:\n",
    "                            counts[t2] = counts[t2] - (bestC - accum) * kernel[t2-t]\n",
    "                        runStart = runStarts[runIdx]\n",
    "                        initIdx = recentIdx + 1\n",
    "                        recentIdx = recentIdx + 1 + t - runStart;\n",
    "\n",
    "                        _skipped = 0\n",
    "                        if recentIdx + 1 > len(recent):\n",
    "                            _skipped = recentIdx - (len(recent) - 1)\n",
    "                            recentIdx = len(recent) - 1\n",
    "\n",
    "\n",
    "                        recent[initIdx:recentIdx + 1] = np.arange(t+1, runStart + _skipped, -1)\n",
    "                        counts[runStart:(t+1)] = \\\n",
    "                               np.concatenate((trace[runStart:t], [bestC])) - \\\n",
    "                               np.concatenate(([0], kernel[1]*trace[runStart:t]))\n",
    "                        done = True\n",
    "                        converged = True\n",
    "                    else: #%C_max will get swamped\n",
    "                        #%in this situation, we know that this point will be removed\n",
    "                        #%as we continue to process the run. To save time:\n",
    "                        t -= 1\n",
    "                        runEnds[runIdx] = t\n",
    "                        accum = accum / kernel[1]\n",
    "                        top = top * kernel[1] + trace[t] #% %this is the correct adjustment to the derivative term above\n",
    "                        bottom = bottom * (kernel[1] ** 2) + 1 #% %this is the correct adjustment to the derivative term above\n",
    "\n",
    "        runIdx -= 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nndSmooth(data, hz, tau, iterFunc=None):\n",
    "    tauSamples = tau * hz\n",
    "\n",
    "    # This is the transient shape we're deconvolving against:\n",
    "    # e^(x/tauSamples), for 8 times the length of tau.\n",
    "    cutoff = round(8 * tauSamples)\n",
    "    fitted = np.exp(-np.arange(cutoff + 1) / tauSamples)\n",
    "\n",
    "    def _singleRowNND(samples):\n",
    "        result = np.copy(samples)\n",
    "        nanSamples = np.isnan(samples)\n",
    "        if np.all(nanSamples):\n",
    "            pass # No data\n",
    "        elif not np.any(nanSamples):\n",
    "            # All samples exist, so fit in one go\n",
    "            result = np.convolve(nonNegativeDeconvolution(samples, tauSamples), fitted)[:len(samples)]\n",
    "        else:\n",
    "            # Lots of different runs of samples, fit each separately\n",
    "            starts = np.where((not nanSamples) & np.isnan(np.concatenate(([1], samples[:-1]))))[0]\n",
    "            ends = np.where((not nanSamples) & np.isnan(np.concatenate((samples[1:], [1]))))[0]\n",
    "            for start, end in zip(starts, ends):\n",
    "                tmp = np.convolve(NND(samples[start:end], tauSamples), fitted)\n",
    "                result[start:end] = np.max(0, tmp[:end - start + 1])\n",
    "        return result\n",
    "\n",
    "    return _forEachTimeseries(data, _singleRowNND, iterFunc)\n",
    "\n",
    "def deltaFOverF0(data, hz, t0=0.2, t1=0.75, t2=3.0, iterFunc=None):\n",
    "    t0ratio = None if t0 is None else np.exp(-1 / (t0 * hz))\n",
    "    t1samples, t2samples = round(t1 * hz), round(t2*hz)\n",
    "\n",
    "    def _singeRowDeltaFOverF(samples):\n",
    "        fBar = _windowFunc(np.mean, samples, t1samples, mid=True)\n",
    "        f0 = _windowFunc(np.min, fBar, t2samples)\n",
    "        result = (samples - f0) / f0\n",
    "        if t0ratio is not None:\n",
    "            result = _ewma(result, t0ratio)\n",
    "        return result\n",
    "    return _forEachTimeseries(data, _singeRowDeltaFOverF, iterFunc)\n",
    "\n",
    "\n",
    "def _windowFunc(f, x, window, mid=False):\n",
    "    n = len(x)\n",
    "    startOffset = (window - 1) // 2 if mid else window - 1\n",
    "    result = np.zeros(x.shape)\n",
    "    for i in range(n):\n",
    "        startIdx = i - startOffset\n",
    "        endIdx = startIdx + window\n",
    "        startIdx, endIdx = max(0, startIdx), min(endIdx, n)\n",
    "        result[i] = f(x[startIdx:endIdx])\n",
    "    return result\n",
    "\n",
    "\n",
    "def _ewma(x, ratio):\n",
    "    result = np.zeros(x.shape)\n",
    "    weightedSum, sumOfWeights = 0.0, 0.0\n",
    "    for i in range(len(x)):\n",
    "        weightedSum = ratio * weightedSum + x[i]\n",
    "        sumOfWeights = ratio * sumOfWeights + 1.0\n",
    "        result[i] = weightedSum / sumOfWeights\n",
    "    return result\n",
    "\n",
    "# Input is either 1d (timeseries), 2d (each row is a timeseries) or 3d (x, y, timeseries)\n",
    "def _forEachTimeseries(data, func, iterFunc=None):\n",
    "    if iterFunc is None:\n",
    "        iterFunc = lambda x: x\n",
    "    dim = len(data.shape)\n",
    "    result = np.zeros(data.shape)\n",
    "    if dim == 1: # single timeseries\n",
    "        result = func(data)\n",
    "    elif dim == 2: # (node, timeseries)\n",
    "        for i in iterFunc(range(data.shape[0])):\n",
    "            result[i] = func(data[i])\n",
    "    elif dim == 3: # (x, y, timeseries)\n",
    "        for i in iterFunc(range(data.shape[0])):\n",
    "            for j in iterFunc(range(data.shape[1])):\n",
    "                result[i, j] = func(data[i, j])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a folder of Tifs and turn it into a numpy array\n",
    "def folder2tif(dir_path):\n",
    "    final = []\n",
    "    files = os.listdir(dir_path)\n",
    "    files = sorted(files)\n",
    "    movie = []\n",
    "    for fname in files:\n",
    "        im = Image.open(os.path.join(dir_path, fname))\n",
    "        imarray = np.array(im)\n",
    "        movie.append(imarray)\n",
    "    movie = np.asarray(movie)\n",
    "    return movie\n",
    "\n",
    "# Returns change in fluorescence over average fluorescence of ROI\n",
    "def deltaF(video_mask):\n",
    "    video_mask_nan = video_mask.copy()\n",
    "    video_mask_nan[video_mask_nan==0] = np.nan\n",
    "    mean = np.mean(np.nanmean(video_mask))\n",
    "    print(mean)\n",
    "    dff= np.zeros((video_mask.shape[0]))\n",
    "    for i in range(dff.shape[0]):\n",
    "        delta = np.nanmean(video_mask[i, :, :])-mean\n",
    "        dff[i] = delta/mean\n",
    "    return dff\n",
    "\n",
    "# Returns raw values fluorescence in the ROI\n",
    "def rawIntensity(video_mask):\n",
    "    video_mask_nan = video_mask.copy()\n",
    "    video_mask_nan[video_mask_nan==0] = np.nan\n",
    "    mean = np.nanmean(video_mask, axis=(1,2))\n",
    "    return mean\n",
    "\n",
    "# Generates the ROI \n",
    "def genROI(gcamp, rcamp):\n",
    "\n",
    "    # To create a ROI for the nucleus a STD projection is created\n",
    "    # Thresholding this image creates a mask for the roi\n",
    "    std_projectionG = np.std(gcamp, axis=0)\n",
    "    threshold = filters.threshold_otsu(std_projectionG)\n",
    "    std_projectionG[std_projectionG < threshold] = 0\n",
    "    std_projectionG[std_projectionG>0]=1\n",
    "\n",
    "    # Create a ROI for the cytosl using an STD projection \n",
    "    # Thresholding this image creates a mask for the roi\n",
    "    std_projectionR = np.std(rcamp, axis=0)\n",
    "    threshold = filters.threshold_otsu(std_projectionR)\n",
    "    std_projectionR[std_projectionR < threshold] = 0\n",
    "    std_projectionR[std_projectionR>0]=1\n",
    "    \n",
    "    # Remove the Nuclear Mask from this ROI\n",
    "    std_projectionR[std_projectionG==1]=0\n",
    "    \n",
    "    # Applying the masks for the two channels\n",
    "    gcamp_masked = gcamp * std_projectionG\n",
    "    rcamp_masked = rcamp * std_projectionR\n",
    "    \n",
    "    return gcamp_masked, rcamp_masked\n",
    "\n",
    "\n",
    "# Generate a ROI for a cell\n",
    "# Input of data directory, cell, Returns RCaMP and GCamP Raw Inrensity\n",
    "def Cell2Trace(path, cell):\n",
    "    # Import the movies provide the path to the folder containing the frames\n",
    "    gcamp = folder2tif(path+cell+\"_G/\")\n",
    "    rcamp = folder2tif(path+cell+\"_R/\")\n",
    "    \n",
    "    # Return Masked Arrays\n",
    "    gcamp_masked, rcamp_masked = genROI(gcamp, rcamp)\n",
    "    \n",
    "    # Return Raw Traces from ROI\n",
    "    gcamp = rawIntensity(gcamp_masked)\n",
    "    rcamp = rawIntensity(rcamp_masked)\n",
    "    \n",
    "    return gcamp, rcamp\n",
    "\n",
    "def peakDetect(trace):\n",
    "    # Detect Peaks\n",
    "    threshold =np.std(trace)\n",
    "    peaks, _ = signal.find_peaks(trace, width=7, rel_height=.5, prominence=(.1*threshold))\n",
    "\n",
    "    width = signal.peak_widths(trace, peaks, rel_height=.1)\n",
    "    \n",
    "    return peaks, width[0]\n",
    "\n",
    "def signal_analysis(cell_id, gcamp, rcamp):\n",
    "    gcamp_peaks, gcamp_widths = peakDetect(gcamp)\n",
    "    rcamp_peaks, rcamp_widths = peakDetect(rcamp)\n",
    "\n",
    "    \n",
    "    # Match Peaks and puttin them in a list of tuples (g, r)\n",
    "    gcamp_matched= []\n",
    "    for g in gcamp_peaks:\n",
    "        for r in rcamp_peaks:\n",
    "            a = math.isclose(g, r, abs_tol=15)\n",
    "            if a == True:\n",
    "                gcamp_matched.append((g,r))\n",
    "                \n",
    "    # Assumption: Last shared peak is due to drug application\n",
    "    # ***Improve this later***\n",
    "    drug_app = np.mean(gcamp_matched[-1])\n",
    "    cutoffG = np.where(gcamp_peaks == gcamp_matched[-1][0])[0][0]\n",
    "    cutoffR = np.where(rcamp_peaks == gcamp_matched[-1][1])[0][0]\n",
    "    gcamp_matched = gcamp_matched[:-1]\n",
    "\n",
    "\n",
    "    # Apply the cutoff \n",
    "    gcamp_peaks = gcamp_peaks[:cutoffG]\n",
    "    gcamp_widths = gcamp_widths[:cutoffG]\n",
    "    rcamp_peaks = rcamp_peaks[:cutoffR]\n",
    "    rcamp_widths = rcamp_widths[:cutoffR]\n",
    "    \n",
    "    \n",
    "    # General Stats for the cell\n",
    "    cell_stats = { \n",
    "                                'Cell ID': cell_id,\n",
    "                                'GCaMP Peaks':len(gcamp_peaks),\n",
    "                                'RCaMP Peaks':len(rcamp_peaks),\n",
    "                                'Shared Peaks':len(gcamp_matched),\n",
    "                                'GCaMP Percent Shared':(len(gcamp_matched)/len(gcamp_peaks)),\n",
    "                                'RCaMP Percent Shared':(len(gcamp_matched)/len(gcamp_peaks)),\n",
    "                                'Experiment Length (s)': gcamp.shape[0]/10,\n",
    "                                'Predicted Drug Application': drug_app/10,\n",
    "                               }\n",
    "    \n",
    "    cell_stats = pd.DataFrame(data=cell_stats, index=[0])\n",
    "    peak_data = pd.DataFrame()\n",
    "    for event in gcamp_matched:\n",
    "        \n",
    "        gindex = np.where(gcamp_peaks == event[0])[0]\n",
    "        rindex = np.where(rcamp_peaks == event[1])[0]\n",
    "        \n",
    "        \n",
    "        # Integrate Under the Curve for Area \n",
    "        # Note: Area from start to peak\n",
    "        g_event_start = int(event[0]-gcamp_widths[gindex])\n",
    "        if g_event_start < 0: \n",
    "            g_event_start = 0\n",
    "        r_event_start = int(event[0]-rcamp_widths[rindex])\n",
    "        if r_event_start < 0:\n",
    "            r_event_start = 0\n",
    "\n",
    "        g_area = integrate.cumtrapz(gcamp[g_event_start:event[0]])\n",
    "        r_area = integrate.cumtrapz(rcamp[r_event_start:event[1]])\n",
    "        \n",
    "        peak_stats = {          'Cell ID': cell_id,            \n",
    "                                'GCaMP Loc':event[0],\n",
    "                                'GCaMP Width':gcamp_widths[gindex],\n",
    "                                'GCaMP Prominence':gcamp[event[0]],\n",
    "                                'GCaMP Area':g_area,                                \n",
    "                                'RCaMP Loc':event[1],\n",
    "                                'RCaMP Width':rcamp_widths[rindex],\n",
    "                                'RCaMP Prominence':rcamp[event[1]],\n",
    "                                'RCaMP Area':r_area,                                \n",
    "                                'Promicence Ratio (G/R)':(gcamp[event[1]]/rcamp[event[0]]),\n",
    "                                'Peak Time Diff (G-r)':((event[0]-event[1])*100),\n",
    "                                                                 }\n",
    "        peak_data = peak_data.append(peak_stats, ignore_index=True)\n",
    "    return cell_stats, peak_data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 14_YAC128_G\n",
      "Cell 14_YAC128_R\n",
      "Cell 17_WT_G\n",
      "Cell 17_WT_R\n",
      "Cell 18_WT_G\n",
      "Cell 18_WT_R\n",
      "Cell 22_YAC128_G\n",
      "Cell 22_YAC128_R\n",
      "Cell 29_YAC128_G\n",
      "Cell 29_YAC128_R\n"
     ]
    }
   ],
   "source": [
    "# Collects all the cells in the analysis data directory and groups\n",
    "# Them by condition in two lists\n",
    "path = os.fspath(DATA_DIRECTORY)\n",
    "cells = sorted((os.listdir(path)))\n",
    "WT_Cells = []\n",
    "YAC128_Cells = []\n",
    "for folder in cells:\n",
    "    print(folder)\n",
    "    if 'WT'  in folder:\n",
    "        WT_Cells.append(folder[:-2])\n",
    "    if 'YAC128'  in folder:\n",
    "        YAC128_Cells.append(folder[:-2])\n",
    "        \n",
    "WT_Cells = np.unique(WT_Cells)\n",
    "YAC128_Cells = np.unique(YAC128_Cells)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h> Cycle Through WT Cells to Extract Peak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nonNegativeDeconvolution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-90ff46d41b85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Calculate df/f and perform NND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdffG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeltaFOverF0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdffG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnndSmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdffG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdffR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeltaFOverF0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdffR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnndSmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdffR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-00df2decbf35>\u001b[0m in \u001b[0;36mnndSmooth\u001b[0;34m(data, hz, tau, iterFunc)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_forEachTimeseries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_singleRowNND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeltaFOverF0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterFunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-00df2decbf35>\u001b[0m in \u001b[0;36m_forEachTimeseries\u001b[0;34m(data, func, iterFunc)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# single timeseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# (node, timeseries)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-00df2decbf35>\u001b[0m in \u001b[0;36m_singleRowNND\u001b[0;34m(samples)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnanSamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# All samples exist, so fit in one go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonNegativeDeconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtauSamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Lots of different runs of samples, fit each separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nonNegativeDeconvolution' is not defined"
     ]
    }
   ],
   "source": [
    "WT_Stats = pd.DataFrame()\n",
    "WT_Peaks = pd.DataFrame()\n",
    "\n",
    "for cell in WT_Cells:\n",
    "    gcamp, rcamp = Cell2Trace(path, cell)\n",
    " \n",
    "    # Calculate df/f and perform NND\n",
    "    dffG = deltaFOverF0(gcamp, HZ)\n",
    "    dffG = nndSmooth(dffG, HZ, tau=1)\n",
    "    dffR = deltaFOverF0(rcamp, HZ)\n",
    "    dffR = nndSmooth(dffR, HZ, tau=1)\n",
    "    \n",
    "    cell_stats, peak_data = signal_analysis(cell, dffG, dffR)\n",
    "    \n",
    "    WT_Stats = WT_Stats.append(cell_stats, ignore_index=True)\n",
    "    WT_Peaks = WT_Peaks.append(peak_data, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Display Results for WT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(WT_Stats)\n",
    "display(WT_Peaks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h> Cycle Through YAC128 Cells to Extract Peak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "YAC128_Stats = pd.DataFrame()\n",
    "YAC128_Peaks = pd.DataFrame()\n",
    "\n",
    "for cell in YAC128_Cells:\n",
    "    gcamp, rcamp = Cell2Trace(path, cell)\n",
    " \n",
    "    # Calculate df/f and perform NND\n",
    "    dffG = pntfilters.deltaFOverF0(gcamp, HZ)\n",
    "    dffG = pntfilters.nndSmooth(dffG, HZ, tau=1)\n",
    "    dffR = pntfilters.deltaFOverF0(rcamp, HZ)\n",
    "    dffR = pntfilters.nndSmooth(dffR, HZ, tau=1)\n",
    "    \n",
    "    cell_stats, peak_data = signal_analysis(cell, dffG, dffR)\n",
    "    \n",
    "    YAC128_Stats = YAC128_Stats.append(cell_stats, ignore_index=True)\n",
    "    YAC128_Peaks = YAC128_Peaks.append(peak_data, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Display Results for YAC128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(YAC128_Stats)\n",
    "display(YAC128_Peaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
