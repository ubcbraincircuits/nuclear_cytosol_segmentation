{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "import skimage.filters as filters\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from PIL import Image\n",
    "#from pyneurotrace import filters  as pntfilters\n",
    "from scipy import integrate\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Folder containing sub-directories to include in analysis\n",
    "DATA_DIRECTORY = \"/home/islandhead/Documents/DBC_Tutoring/Wissam/osfstorage-archive/Data (Images)/\"\n",
    "# Frequency frames were collected\n",
    "HZ = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Jia, H., Rochefort, N. L., Chen, X., & Konnerth, A. (2011).\n",
    "In vivo two-photon imaging of sensory-evoked dendritic calcium signals in cortical neurons.\n",
    "Nature protocols, 6(1), 28.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Podgorski, K., & Haas, K. (2013).\n",
    "Fast non‐negative temporal deconvolution for laser scanning microscopy.\n",
    "Journal of biophotonics, 6(2), 153-162.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Coleman, P. (2019).\n",
    "pyNeuroTrace. https://github.com/padster/pyNeuroTrace.git\n",
    "\"\"\"\n",
    "# To install pyneuortrace use this:\n",
    "# pip install --upgrade \"git+https://github.com/padster/pyNeuroTrace#egg=pyneurotrace&subdirectory=pyneurotrace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change output figure size\n",
    "# ...needs to be in its own cell for some reason...\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Performs fast nonnegative deconvolution on pmt signal to solve for minimum MSE photon rate\n",
    "   trace  :   The data to be deconvolved\n",
    "   tau    :   The time constant of the PMT, in data samples\n",
    "   return :   estimated photon rate\n",
    "A matlab version is also available on request.\n",
    "For details on how this works, see:\n",
    "  Podgorski, K., & Haas, K. (2013).\n",
    "  Fast non‐negative temporal deconvolution for laser scanning microscopy.\n",
    "  Journal of biophotonics, 6(2), 153-162.\n",
    "\"\"\"\n",
    "def nonNegativeDeconvolution(trace, tau):\n",
    "    T = len(trace)\n",
    "    counts = np.zeros(T)\n",
    "    counts[-1] = trace[-1]\n",
    "    cutoff = math.ceil(8 * tau)\n",
    "    kernel = np.exp(-np.arange(cutoff + 1)/tau) # convolution kernel\n",
    "    recent = np.full(1 + round(T / 2), np.nan).astype(int)\n",
    "    recent[0] = T #stored locations where we assigned counts\n",
    "    recentIdx = 0\n",
    "\n",
    "    # the points that could potentially be assigned counts:\n",
    "    _delayed = np.concatenate(([0], trace[:-2]))\n",
    "    points = (trace[:-1] > kernel[1] * _delayed) & (trace[:-1] > 0)\n",
    "\n",
    "    # dividing these points up into runs, for speed\n",
    "    runStarts = np.where(points & ~(np.concatenate(([False], points[:-1]))))[0].astype(int)\n",
    "    runEnds = np.where(points & ~(np.concatenate((points[1:], [False]))))[0].astype(int)\n",
    "    runIdx = len(runEnds) - 1\n",
    "\n",
    "    while runIdx >= 0:\n",
    "        oldTop, oldBottom = 0, 0\n",
    "        t = runEnds[runIdx]\n",
    "        t1 = t\n",
    "        accum = 0\n",
    "\n",
    "        converged = False\n",
    "        while not converged:\n",
    "            if recentIdx >= 0 and recent[recentIdx] < (t+cutoff):\n",
    "                t2 = recent[recentIdx] - 1\n",
    "                C_max = counts[t2] / kernel[t2-t]\n",
    "            else:\n",
    "                t2 = min(t + cutoff, T+1) - 1\n",
    "                C_max = np.inf\n",
    "\n",
    "\n",
    "            b = kernel[t1-t:t2-t]\n",
    "            top = np.dot(b, trace[t1:t2]) + oldTop #this is the numerator of the least squares fit for an exponential\n",
    "            bottom = np.dot(b, b) + oldBottom #this is the denominator of the fit\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                #the error function is (data-kernel.*C)^2\n",
    "                bestC = max(top/bottom, 0);  #C=top/bottom sets the derivative of the error to 0\n",
    "\n",
    "                # does not meet nonnegative constraint. Continue to adjust previous solutions.\n",
    "                if bestC > (C_max+accum):\n",
    "                    accum = accum + counts[t2] / kernel[t2-t]\n",
    "                    counts[t2] = 0\n",
    "                    t1 = t2\n",
    "                    oldTop = top\n",
    "                    oldBottom = bottom\n",
    "                    recentIdx -= 1\n",
    "                    done = True\n",
    "\n",
    "                else: # converged!\n",
    "                    #now that we have found the MSE counts for times t<end, check if\n",
    "                    #this will be swamped by the next timepoint in the run\n",
    "                    if  (t == runStarts[runIdx]) or (trace[t-1] < bestC/kernel[1]): #%C_max won't necessarily get swamped\n",
    "                        if recentIdx >= 0 and t2 <= t + cutoff:\n",
    "                            counts[t2] = counts[t2] - (bestC - accum) * kernel[t2-t]\n",
    "                        runStart = runStarts[runIdx]\n",
    "                        initIdx = recentIdx + 1\n",
    "                        recentIdx = recentIdx + 1 + t - runStart;\n",
    "\n",
    "                        _skipped = 0\n",
    "                        if recentIdx + 1 > len(recent):\n",
    "                            _skipped = recentIdx - (len(recent) - 1)\n",
    "                            recentIdx = len(recent) - 1\n",
    "\n",
    "\n",
    "                        recent[initIdx:recentIdx + 1] = np.arange(t+1, runStart + _skipped, -1)\n",
    "                        counts[runStart:(t+1)] = \\\n",
    "                               np.concatenate((trace[runStart:t], [bestC])) - \\\n",
    "                               np.concatenate(([0], kernel[1]*trace[runStart:t]))\n",
    "                        done = True\n",
    "                        converged = True\n",
    "                    else: #%C_max will get swamped\n",
    "                        #%in this situation, we know that this point will be removed\n",
    "                        #%as we continue to process the run. To save time:\n",
    "                        t -= 1\n",
    "                        runEnds[runIdx] = t\n",
    "                        accum = accum / kernel[1]\n",
    "                        top = top * kernel[1] + trace[t] #% %this is the correct adjustment to the derivative term above\n",
    "                        bottom = bottom * (kernel[1] ** 2) + 1 #% %this is the correct adjustment to the derivative term above\n",
    "\n",
    "        runIdx -= 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nndSmooth(data, hz, tau, iterFunc=None):\n",
    "    tauSamples = tau * hz\n",
    "\n",
    "    # This is the transient shape we're deconvolving against:\n",
    "    # e^(x/tauSamples), for 8 times the length of tau.\n",
    "    cutoff = round(8 * tauSamples)\n",
    "    fitted = np.exp(-np.arange(cutoff + 1) / tauSamples)\n",
    "\n",
    "    def _singleRowNND(samples):\n",
    "        result = np.copy(samples)\n",
    "        nanSamples = np.isnan(samples)\n",
    "        if np.all(nanSamples):\n",
    "            pass # No data\n",
    "        elif not np.any(nanSamples):\n",
    "            # All samples exist, so fit in one go\n",
    "            result = np.convolve(nonNegativeDeconvolution(samples, tauSamples), fitted)[:len(samples)]\n",
    "        else:\n",
    "            # Lots of different runs of samples, fit each separately\n",
    "            starts = np.where((not nanSamples) & np.isnan(np.concatenate(([1], samples[:-1]))))[0]\n",
    "            ends = np.where((not nanSamples) & np.isnan(np.concatenate((samples[1:], [1]))))[0]\n",
    "            for start, end in zip(starts, ends):\n",
    "                tmp = np.convolve(NND(samples[start:end], tauSamples), fitted)\n",
    "                result[start:end] = np.max(0, tmp[:end - start + 1])\n",
    "        return result\n",
    "\n",
    "    return _forEachTimeseries(data, _singleRowNND, iterFunc)\n",
    "\n",
    "def deltaFOverF0(data, hz, t0=0.2, t1=0.75, t2=3.0, iterFunc=None):\n",
    "    t0ratio = None if t0 is None else np.exp(-1 / (t0 * hz))\n",
    "    t1samples, t2samples = round(t1 * hz), round(t2*hz)\n",
    "\n",
    "    def _singeRowDeltaFOverF(samples):\n",
    "        fBar = _windowFunc(np.mean, samples, t1samples, mid=True)\n",
    "        f0 = _windowFunc(np.min, fBar, t2samples)\n",
    "        result = (samples - f0) / f0\n",
    "        if t0ratio is not None:\n",
    "            result = _ewma(result, t0ratio)\n",
    "        return result\n",
    "    return _forEachTimeseries(data, _singeRowDeltaFOverF, iterFunc)\n",
    "\n",
    "\n",
    "def _windowFunc(f, x, window, mid=False):\n",
    "    n = len(x)\n",
    "    startOffset = (window - 1) // 2 if mid else window - 1\n",
    "    result = np.zeros(x.shape)\n",
    "    for i in range(n):\n",
    "        startIdx = i - startOffset\n",
    "        endIdx = startIdx + window\n",
    "        startIdx, endIdx = max(0, startIdx), min(endIdx, n)\n",
    "        result[i] = f(x[startIdx:endIdx])\n",
    "    return result\n",
    "\n",
    "\n",
    "def _ewma(x, ratio):\n",
    "    result = np.zeros(x.shape)\n",
    "    weightedSum, sumOfWeights = 0.0, 0.0\n",
    "    for i in range(len(x)):\n",
    "        weightedSum = ratio * weightedSum + x[i]\n",
    "        sumOfWeights = ratio * sumOfWeights + 1.0\n",
    "        result[i] = weightedSum / sumOfWeights\n",
    "    return result\n",
    "\n",
    "# Input is either 1d (timeseries), 2d (each row is a timeseries) or 3d (x, y, timeseries)\n",
    "def _forEachTimeseries(data, func, iterFunc=None):\n",
    "    if iterFunc is None:\n",
    "        iterFunc = lambda x: x\n",
    "    dim = len(data.shape)\n",
    "    result = np.zeros(data.shape)\n",
    "    if dim == 1: # single timeseries\n",
    "        result = func(data)\n",
    "    elif dim == 2: # (node, timeseries)\n",
    "        for i in iterFunc(range(data.shape[0])):\n",
    "            result[i] = func(data[i])\n",
    "    elif dim == 3: # (x, y, timeseries)\n",
    "        for i in iterFunc(range(data.shape[0])):\n",
    "            for j in iterFunc(range(data.shape[1])):\n",
    "                result[i, j] = func(data[i, j])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a folder of Tifs and turn it into a numpy array\n",
    "def folder2tif(dir_path):\n",
    "    final = []\n",
    "    files = os.listdir(dir_path)\n",
    "    files = sorted(files)\n",
    "    movie = []\n",
    "    for fname in files:\n",
    "        im = Image.open(os.path.join(dir_path, fname))\n",
    "        imarray = np.array(im)\n",
    "        movie.append(imarray)\n",
    "    movie = np.asarray(movie)\n",
    "    return movie\n",
    "\n",
    "# Returns change in fluorescence over average fluorescence of ROI\n",
    "def deltaF(video_mask):\n",
    "    video_mask_nan = video_mask.copy()\n",
    "    video_mask_nan[video_mask_nan==0] = np.nan\n",
    "    mean = np.mean(np.nanmean(video_mask))\n",
    "    print(mean)\n",
    "    dff= np.zeros((video_mask.shape[0]))\n",
    "    for i in range(dff.shape[0]):\n",
    "        delta = np.nanmean(video_mask[i, :, :])-mean\n",
    "        dff[i] = delta/mean\n",
    "    return dff\n",
    "\n",
    "# Returns raw values fluorescence in the ROI\n",
    "def rawIntensity(video_mask):\n",
    "    video_mask_nan = video_mask.copy()\n",
    "    video_mask_nan[video_mask_nan==0] = np.nan\n",
    "    mean = np.nanmean(video_mask, axis=(1,2))\n",
    "    return mean\n",
    "\n",
    "# Generates the ROI \n",
    "def genROI(gcamp, rcamp):\n",
    "\n",
    "    # To create a ROI for the nucleus a STD projection is created\n",
    "    # Thresholding this image creates a mask for the roi\n",
    "    std_projectionG = np.std(gcamp, axis=0)\n",
    "    threshold = filters.threshold_otsu(std_projectionG)\n",
    "    std_projectionG[std_projectionG < threshold] = 0\n",
    "    std_projectionG[std_projectionG>0]=1\n",
    "\n",
    "    # Create a ROI for the cytosl using an STD projection \n",
    "    # Thresholding this image creates a mask for the roi\n",
    "    std_projectionR = np.std(rcamp, axis=0)\n",
    "    threshold = filters.threshold_otsu(std_projectionR)\n",
    "    std_projectionR[std_projectionR < threshold] = 0\n",
    "    std_projectionR[std_projectionR>0]=1\n",
    "    \n",
    "    # Remove the Nuclear Mask from this ROI\n",
    "    std_projectionR[std_projectionG==1]=0\n",
    "    \n",
    "    # Applying the masks for the two channels\n",
    "    gcamp_masked = gcamp * std_projectionG\n",
    "    rcamp_masked = rcamp * std_projectionR\n",
    "    \n",
    "    return gcamp_masked, rcamp_masked\n",
    "\n",
    "\n",
    "# Generate a ROI for a cell\n",
    "# Input of data directory, cell, Returns RCaMP and GCamP Raw Inrensity\n",
    "def Cell2Trace(path, cell):\n",
    "    # Import the movies provide the path to the folder containing the frames\n",
    "    gcamp = folder2tif(path+cell+\"_G/\")\n",
    "    rcamp = folder2tif(path+cell+\"_R/\")\n",
    "    \n",
    "    # Return Masked Arrays\n",
    "    gcamp_masked, rcamp_masked = genROI(gcamp, rcamp)\n",
    "    \n",
    "    # Return Raw Traces from ROI\n",
    "    gcamp = rawIntensity(gcamp_masked)\n",
    "    rcamp = rawIntensity(rcamp_masked)\n",
    "    \n",
    "    return gcamp, rcamp\n",
    "\n",
    "def peakDetect(trace, endpoint):\n",
    "    # Third Order Butterworth lowpass filter; 3hz cutoff\n",
    "    \n",
    "    fc = 3  # Cut-off frequency of the filter\n",
    "    w = fc / (10 / 2) # Normalize the frequency      \n",
    "    b, a = signal.butter(3, w, 'low', analog=True)\n",
    "    z = signal.lfilter(b, a, trace, axis=0)\n",
    "    \n",
    "    #plt.plot(trace, linewidth=2, color='Black')\n",
    "    #plt.plot(z, linewidth=2, color='Red')\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    # Detect Peaks\n",
    "    threshold = np.std(z[:endpoint])\n",
    "\n",
    "    peaks, _ = signal.find_peaks(trace, width=7, rel_height=.5, prominence=(threshold))\n",
    "\n",
    "    width = signal.peak_widths(trace, peaks, rel_height=.1)\n",
    "    \n",
    "    return peaks, width[0]\n",
    "\n",
    "def signal_analysis(cell_id, gcamp, rcamp, rawG, rawR):\n",
    "    print(cell_id)\n",
    "    str_index = int(cell_id.find(\"Iono_\"))\n",
    "    threshold_cuttoff = (int(cell_id[(str_index+5):(str_index+8)])*HZ)\n",
    "    gcamp_peaks, gcamp_widths = peakDetect(gcamp, threshold_cuttoff)\n",
    "    rcamp_peaks, rcamp_widths = peakDetect(rcamp, threshold_cuttoff)\n",
    "\n",
    "\n",
    "    drug_app = np.nan\n",
    "    iono_min = [np.nan, np.nan]\n",
    "    iono_max = [np.nan, np.nan]\n",
    "    iono_diff = [np.nan, np.nan]\n",
    "    \n",
    "    # Update: Now uses value of drug app from folder name\n",
    "    # Should return -1 is no match is found for the key string 'Iono_'\n",
    "    if cell_id.find(\"Iono_\") is not -1:\n",
    "        str_index = int(cell_id.find(\"Iono_\"))\n",
    "        drug_app = (int(cell_id[(str_index+5):(str_index+8)])*HZ)\n",
    "        cutoffG = np.array(np.where(gcamp_peaks >= drug_app)[0])\n",
    "        if cutoffG.size !=0:\n",
    "            cutoffG = np.min(cutoffG)\n",
    "            \n",
    "        if cutoffG != 0:\n",
    "\n",
    "            if cutoffG.size !=0:\n",
    "                cutoffG = np.min(cutoffG)\n",
    "\n",
    "            cutoffR = np.array(np.where(rcamp_peaks >= drug_app))\n",
    "            if cutoffR.size !=0:\n",
    "                cutoffR = np.min(cutoffR)\n",
    " \n",
    "            gcamp_peaks = gcamp_peaks[:cutoffG]\n",
    "            gcamp_widths = gcamp_widths[:cutoffG]\n",
    "            rcamp_peaks = rcamp_peaks[:cutoffR]\n",
    "            rcamp_widths = rcamp_widths[:cutoffR]\n",
    "    \n",
    "  \n",
    "    if cell_id.find(\"_S\") is not -1:\n",
    "        if drug_app is not np.nan:\n",
    "            iono_min[0] = np.min(rawG[drug_app-200:drug_app+1000]>0)\n",
    "            iono_min[1] = np.min(rawR[drug_app-200:drug_app+1000]>0)\n",
    "\n",
    "            iono_max[0] = np.max(rawG[drug_app-200:drug_app+1000])\n",
    "            iono_max[1] = np.max(rawR[drug_app-200:drug_app+1000])\n",
    "\n",
    "            iono_diff[0] = iono_max[0]-iono_min[0]\n",
    "            iono_diff[1] = iono_max[1]-iono_min[1]\n",
    "\n",
    "    \n",
    "    # Match Peaks and puttin them in a list of tuples (g, r)\n",
    "    gcamp_matched = []\n",
    "    shared_rcamp = []\n",
    "    shared_gcamp = []\n",
    "    for g in gcamp_peaks:\n",
    "        for r in rcamp_peaks:\n",
    "            a = math.isclose(g, r, abs_tol=15)\n",
    "            if a == True:\n",
    "                gcamp_matched.append((g,r))\n",
    "                shared_rcamp.append(r)\n",
    "                shared_gcamp.append(g)\n",
    "    \n",
    "    rcamp_only = list(set(rcamp_peaks)-set(shared_rcamp))\n",
    "    rcamp_only.sort\n",
    "    gcamp_only = list(set(gcamp_peaks)-set(shared_gcamp))\n",
    "    gcamp_only.sort\n",
    "\n",
    "    if len(gcamp_peaks) == 0:\n",
    "        g_percent_shared = np.nan\n",
    "    else:\n",
    "        g_percent_shared = (len(gcamp_matched)/len(gcamp_peaks))\n",
    "        \n",
    "    if len(rcamp_peaks) == 0:\n",
    "        r_percent_shared = np.nan\n",
    "    else:\n",
    "        r_percent_shared = (len(gcamp_matched)/len(rcamp_peaks))\n",
    "    if len(gcamp_matched) == 0:\n",
    "        r_percent_shared = np.nan\n",
    "        g_percent_shared = np.nan\n",
    "        \n",
    "    # General Stats for the cell\n",
    "    cell_stats = { \n",
    "                                'Cell ID': cell_id,\n",
    "                                'GCaMP Peaks':len(gcamp_peaks),\n",
    "                                'RCaMP Peaks':len(rcamp_peaks),\n",
    "                                'Shared Peaks':len(gcamp_matched),\n",
    "                                'GCaMP Percent Shared':g_percent_shared,\n",
    "                                'RCaMP Percent Shared':r_percent_shared,\n",
    "                                'Experiment Length (s)': gcamp.shape[0]/10,\n",
    "                                'Drug Application': drug_app/10,\n",
    "                                'Iono GCaMP Max': iono_max[0],\n",
    "                                'Iono GCaMP Dif': iono_diff[0],\n",
    "                                'Iono RCaMP Max': iono_max[1],\n",
    "                                'Iono RCaMP Dif': iono_diff[1],\n",
    "\n",
    "                                \n",
    "                               }\n",
    "    \n",
    "    cell_stats = pd.DataFrame(data=cell_stats, index=[0])\n",
    "    \n",
    "    # Peak Data for Shared Peaks\n",
    "    shared_peak_data = pd.DataFrame()\n",
    "    for event in gcamp_matched:\n",
    "        \n",
    "        gindex = np.where(gcamp_peaks == event[0])[0]\n",
    "        rindex = np.where(rcamp_peaks == event[1])[0]\n",
    "        \n",
    "        \n",
    "        # Integrate Under the Curve for Area \n",
    "        # Note: Area from start to peak\n",
    "        g_event_start = int(event[0]-gcamp_widths[gindex])\n",
    "        if g_event_start < 0: \n",
    "            g_event_start = 0\n",
    "        r_event_start = int(event[0]-rcamp_widths[rindex])\n",
    "        if r_event_start < 0:\n",
    "            r_event_start = 0\n",
    "\n",
    "        g_area = integrate.cumtrapz(gcamp[g_event_start:event[0]])\n",
    "        if len(g_area) is not 0:\n",
    "            g_area = g_area[-1]\n",
    "        r_area = integrate.cumtrapz(rcamp[r_event_start:event[1]])\n",
    "        if len(r_area) is not 0:\n",
    "            r_area = r_area[-1]\n",
    "        \n",
    "        peak_stats = {          'Cell ID': cell_id,            \n",
    "                                'GCaMP Loc':event[0],\n",
    "                                'GCaMP Start': event[0]-gcamp_widths[gindex][0]*HZ,\n",
    "                                'GCaMP Width':gcamp_widths[gindex][0],\n",
    "                                'GCaMP Prominence':gcamp[event[0]],\n",
    "                                'GCaMP Area':g_area,                                \n",
    "                                'RCaMP Loc':event[1],\n",
    "                                'RCaMP Start': event[1]-rcamp_widths[rindex][0]*HZ,\n",
    "                                'RCaMP Width':rcamp_widths[rindex][0],\n",
    "                                'RCaMP Prominence':rcamp[event[1]],\n",
    "                                'RCaMP Area':r_area,                                \n",
    "                                'Promicence Ratio (G/R)':(gcamp[event[1]]/rcamp[event[0]]),\n",
    "                                'Peak Time Diff (G-R)':((event[0]-event[1])*100),\n",
    "                                'Start Difference (G-R)': (event[0]-gcamp_widths[gindex] - event[1]-rcamp_widths[rindex])[0]*100  \n",
    "                                                                 }\n",
    "        shared_peak_data = shared_peak_data.append(peak_stats, ignore_index=True)\n",
    "    \n",
    "    # Adding RCaMP peaks to the shared datatable\n",
    "    for event in rcamp_only:\n",
    "        \n",
    "        rindex = np.where(rcamp_peaks == event)[0]\n",
    "        \n",
    "        \n",
    "        # Integrate Under the Curve for Area \n",
    "        # Note: Area from start to peak\n",
    "        r_event_start = int(event-rcamp_widths[rindex])\n",
    "        if r_event_start < 0:\n",
    "            r_event_start = 0\n",
    "\n",
    "        r_area = integrate.cumtrapz(rcamp[r_event_start:event])\n",
    "        if len(r_area) is not 0:\n",
    "            r_area = r_area[-1]\n",
    "        \n",
    "        peak_stats = {          'Cell ID': cell_id,            \n",
    "                                'GCaMP Loc':np.nan,\n",
    "                                'GCaMP Start': np.nan,\n",
    "                                'GCaMP Width':np.nan,\n",
    "                                'GCaMP Prominence':np.nan,\n",
    "                                'GCaMP Area':np.nan,                                \n",
    "                                'RCaMP Loc':event,\n",
    "                                'RCaMP Start': event-rcamp_widths[rindex][0]*HZ,\n",
    "                                'RCaMP Width':rcamp_widths[rindex][0],\n",
    "                                'RCaMP Prominence':rcamp[event],\n",
    "                                'RCaMP Area':r_area,                                \n",
    "                                'Promicence Ratio (G/R)':0,\n",
    "                                'Peak Time Diff (G-R)':np.nan,\n",
    "                                'Start Difference (G-R)': np.nan,  \n",
    "                                                                 }\n",
    "        shared_peak_data = shared_peak_data.append(peak_stats, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Peak Data for exclusive GCaMP Peaks\n",
    "    gcamp_peak_data = pd.DataFrame()\n",
    "    for event in gcamp_only:\n",
    "        gindex = np.where(gcamp_peaks == event)[0]\n",
    "\n",
    "        # Integrate Under the Curve for Area \n",
    "        # Note: Area from start to peak\n",
    "        g_event_start = int(event-gcamp_widths[gindex])\n",
    "        if g_event_start < 0:\n",
    "            g_event_start = 0\n",
    "\n",
    "        g_area = integrate.cumtrapz(gcamp[g_event_start:event])\n",
    "        if len(g_area) is not 0:\n",
    "            g_area = g_area[-1]\n",
    "        \n",
    "        peak_stats = {          'Cell ID': cell_id,            \n",
    "                                'GCaMP Loc':event,\n",
    "                                'GCaMP Start': event-gcamp_widths[gindex][0]*HZ,\n",
    "                                'GCaMP Width':gcamp_widths[gindex][0],\n",
    "                                'GCaMP Prominence':gcamp[event],\n",
    "                                'GCaMP Area':g_area,\n",
    "                                                         }\n",
    "                                                         \n",
    "        gcamp_peak_data = gcamp_peak_data.append(peak_stats, ignore_index=True) \n",
    "        \n",
    "    # Peak Data for exclusive RCaMP Peaks\n",
    "    rcamp_peak_data = pd.DataFrame()\n",
    "                                                         \n",
    "    for event in rcamp_only:\n",
    "        rindex = np.where(rcamp_peaks == event)[0]\n",
    "        # Integrate Under the Curve for Area \n",
    "        # Note: Area from start to peak\n",
    "        r_event_start = int(event-rcamp_widths[rindex])\n",
    "        if r_event_start < 0:\n",
    "            r_event_start = 0\n",
    "\n",
    "        r_area = integrate.cumtrapz(rcamp[r_event_start:event])\n",
    "        if len(r_area) is not 0:\n",
    "            r_area = r_area[-1]\n",
    "        \n",
    "        peak_stats = {          'Cell ID': cell_id,            \n",
    "                                'RCaMP Loc':event,\n",
    "                                'RCaMP Start': (event-rcamp_widths[rindex][0])*HZ,\n",
    "                                'RCaMP Width':rcamp_widths[rindex][0],\n",
    "                                'RCaMP Prominence':rcamp[event],\n",
    "                                'RCaMP Area':r_area, \n",
    "\n",
    "                                                                 }\n",
    "        rcamp_peak_data = rcamp_peak_data.append(peak_stats, ignore_index=True) \n",
    "        \n",
    "        \n",
    "        cell_stats[\"Prominence Ratio Mean\"] = np.nanmean(np.array(shared_peak_data[\"Promicence Ratio (G/R)\"]))\n",
    "        cell_stats[\"Prominence Ratio SEM\"] = sem(np.array(shared_peak_data[\"Promicence Ratio (G/R)\"]))\n",
    "        \n",
    "    return cell_stats, shared_peak_data, gcamp_peak_data, rcamp_peak_data\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(cell, RTrace, GTrace, Rdff, Gdff ):\n",
    "    plt.rcParams[\"figure.figsize\"] = [16, 10]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2)\n",
    "    ax[0].plot(RTrace.loc[cell][1], linewidth=1, color='Red')\n",
    "    ax[1].plot(Rdff.loc[cell][1], linewidth=1, color='Red')\n",
    "    ax[0].plot(GTrace.loc[cell][1], linewidth=1, color='Green')\n",
    "    ax[1].plot(Gdff.loc[cell][1], linewidth=1, color='Green')\n",
    "    ax[0].set_ylabel('Raw Intensity')\n",
    "    ax[1].set_ylabel(' ΔF/F ')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects all the cells in the analysis data directory and groups\n",
    "# Them by condition in two lists\n",
    "path = os.fspath(DATA_DIRECTORY)\n",
    "cells = sorted((os.listdir(path)))\n",
    "WT_Cells = []\n",
    "YAC128_Cells = []\n",
    "for folder in cells:\n",
    "    print(folder)\n",
    "    if 'WT'  in folder:\n",
    "        WT_Cells.append(folder[:-2])\n",
    "    if 'YAC128'  in folder:\n",
    "        YAC128_Cells.append(folder[:-2])\n",
    "        \n",
    "WT_Cells = np.unique(WT_Cells)\n",
    "YAC128_Cells = np.unique(YAC128_Cells)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h> Cycle Through WT Cells to Extract Peak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WT_Stats = pd.DataFrame()\n",
    "WT_Shared_Peaks = pd.DataFrame()\n",
    "WT_GCaMP = pd.DataFrame()\n",
    "WT_RCaMP = pd.DataFrame()\n",
    "\n",
    "WT_GTrace = pd.DataFrame()\n",
    "WT_RTrace = pd.DataFrame()\n",
    "\n",
    "WT_GDFF = pd.DataFrame()\n",
    "WT_RDFF = pd.DataFrame()\n",
    "\n",
    "for cell in WT_Cells:\n",
    "    gcamp, rcamp = Cell2Trace(path, cell)\n",
    "    \n",
    "    WT_RTrace = WT_RTrace.append(pd.Series([cell, rcamp], name=cell))\n",
    "    WT_GTrace = WT_GTrace.append(pd.Series([cell, gcamp], name=cell))   \n",
    "        \n",
    "    # Calculate df/f and perform NND\n",
    "    dffG = deltaFOverF0(gcamp, HZ)\n",
    "    dffG = nndSmooth(dffG, HZ, tau=1)\n",
    "    dffR = deltaFOverF0(rcamp, HZ)\n",
    "    dffR = nndSmooth(dffR, HZ, tau=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    WT_RDFF = WT_RDFF.append(pd.Series([cell, dffR], name=cell))\n",
    "    WT_GDFF = WT_GDFF.append(pd.Series([cell, dffG], name=cell))\n",
    "    \n",
    "    cell_stats, peak_data, gcamp_peak_data, rcamp_peak_data= signal_analysis(cell, dffG, dffR, gcamp, rcamp)\n",
    "    \n",
    "    WT_Stats = WT_Stats.append(cell_stats, ignore_index=True)\n",
    "    WT_Shared_Peaks = WT_Shared_Peaks.append(peak_data, ignore_index=True)\n",
    "    WT_GCaMP = WT_GCaMP.append(gcamp_peak_data, ignore_index=True)\n",
    "    WT_RCaMP = WT_RCaMP.append(rcamp_peak_data, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Save Results for WT to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=8000)\n",
    "WT_Stats.to_csv(\"WT_Stats.csv\", index=False)\n",
    "WT_Shared_Peaks.to_csv(\"WT_Shared_Peaks.csv\", index=False)\n",
    "WT_GCaMP.to_csv(\"WT_GCaMP.csv\", index=False)\n",
    "WT_RCaMP.to_csv(\"WT_RCaMP.csv\", index=False)\n",
    "\n",
    "WT_RTrace.to_csv(\"WT_R_Trace.csv\", index=False)\n",
    "WT_GTrace.to_csv(\"WT_G_Trace.csv\", index=False)\n",
    "WT_RDFF.to_csv(\"WT_R_DFF.csv\", index=False)\n",
    "WT_GDFF.to_csv(\"WT_G_DFF.csv\", index=False)\n",
    "\n",
    "np.set_printoptions(threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Display Results for WT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(WT_Stats)\n",
    "display(WT_Shared_Peaks)\n",
    "display(WT_GCaMP)\n",
    "display(WT_RCaMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectWTCells = widgets.Dropdown(\n",
    "    options=WT_Cells,\n",
    "    value=WT_Cells[0],\n",
    "    description='WT Cells:',\n",
    "    disabled=False,\n",
    ")\n",
    "def on_change(change, results=results):\n",
    "    if change['name'] == 'value' and (change['new'] != change['old']):\n",
    "        clear_output()\n",
    "        display(SelectWTCells)\n",
    "        print(change['new'])\n",
    "        results(change['new'], WT_RTrace, WT_GTrace, WT_RDFF, WT_GDFF)\n",
    "     \n",
    "        \n",
    "display(SelectWTCells)\n",
    "results(WT_Cells[0], WT_RTrace, WT_GTrace, WT_RDFF, WT_GDFF)\n",
    "SelectWTCells.observe(on_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h> Cycle Through YAC128 Cells to Extract Peak Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "YAC128_Stats = pd.DataFrame()\n",
    "YAC128_Shared_Peaks = pd.DataFrame()\n",
    "YAC128_GCaMP = pd.DataFrame()\n",
    "YAC128_RCaMP = pd.DataFrame()\n",
    "\n",
    "YAC128_GTrace = pd.DataFrame()\n",
    "YAC128_RTrace = pd.DataFrame()\n",
    "\n",
    "YAC128_GDFF = pd.DataFrame()\n",
    "YAC128_RDFF = pd.DataFrame()\n",
    "for cell in YAC128_Cells:\n",
    "    gcamp, rcamp = Cell2Trace(path, cell)\n",
    "    \n",
    "    YAC128_RTrace = YAC128_RTrace.append(pd.Series([cell, rcamp], name=cell))\n",
    "    YAC128_GTrace = YAC128_GTrace.append(pd.Series([cell, gcamp], name=cell))   \n",
    "        \n",
    "\n",
    "    # Calculate df/f and perform NND\n",
    "    dffG = deltaFOverF0(gcamp, HZ)\n",
    "    dffG = nndSmooth(dffG, HZ, tau=1)\n",
    "    dffR = deltaFOverF0(rcamp, HZ)\n",
    "    dffR = nndSmooth(dffR, HZ, tau=1)\n",
    "    \n",
    "    \n",
    "    YAC128_RDFF = YAC128_RDFF.append(pd.Series([cell, dffR], name=cell))\n",
    "    YAC128_GDFF = YAC128_GDFF.append(pd.Series([cell, dffG], name=cell))\n",
    "    \n",
    "    \n",
    "    cell_stats, peak_data, gcamp_peak_data, rcamp_peak_data = signal_analysis(cell, dffG, dffR, gcamp, rcamp)\n",
    "    \n",
    "    \n",
    "    YAC128_Stats = YAC128_Stats.append(cell_stats, ignore_index=True)\n",
    "    YAC128_Shared_Peaks = YAC128_Shared_Peaks.append(peak_data, ignore_index=True)\n",
    "    YAC128_GCaMP = YAC128_GCaMP.append(gcamp_peak_data, ignore_index=True)\n",
    "    YAC128_RCaMP = YAC128_RCaMP.append(rcamp_peak_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Save Results for YAC128 to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=8000)\n",
    "YAC128_Stats.to_csv(\"YAC128_Stats.csv\", index=False)\n",
    "YAC128_Shared_Peaks.to_csv(\"YAC128_Shared_Peaks.csv\", index=False)\n",
    "YAC128_GCaMP.to_csv(\"YAC128_GCaMP.csv\", index=False)\n",
    "YAC128_RCaMP.to_csv(\"YAC128_RCaMP.csv\", index=False)\n",
    "\n",
    "YAC128_RTrace.to_csv(\"YAC128_R_Trace.csv\", index=False)\n",
    "YAC128_GTrace.to_csv(\"YAC128_G_Trace.csv\", index=False)\n",
    "YAC128_RDFF.to_csv(\"YAC128_R_DFF.csv\", index=False)\n",
    "YAC128_GDFF.to_csv(\"YAC128_G_DFF.csv\", index=False)\n",
    "\n",
    "np.set_printoptions(threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Display Results for YAC128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(YAC128_Stats)\n",
    "display(YAC128_Shared_Peaks)\n",
    "display(YAC128_GCaMP)\n",
    "display(YAC128_RCaMP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2> Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectYAC128Cells = widgets.Dropdown(\n",
    "    options=YAC128_Cells,\n",
    "    value=YAC128_Cells[0],\n",
    "    description='YAC128 Cell:',\n",
    "    disabled=False,\n",
    ")\n",
    "def on_change(change, results=results):\n",
    "    if change['name'] == 'value' and (change['new'] != change['old']):\n",
    "        clear_output()\n",
    "        display(SelectYAC128Cells)\n",
    "        print(change['new'])\n",
    "        results(change['new'], YAC128_RTrace, YAC128_GTrace, YAC128_RDFF, YAC128_GDFF)\n",
    "     \n",
    "        \n",
    "display(SelectYAC128Cells)\n",
    "results(YAC128_Cells[0], YAC128_RTrace, YAC128_GTrace, YAC128_RDFF, YAC128_GDFF)\n",
    "SelectYAC128Cells.observe(on_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
